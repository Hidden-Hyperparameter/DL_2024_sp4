import copy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self, args):
        super().__init__()
        ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        self.largeNum = 122753
        self.device = args.device
        global llm
        self.path = '/ssdshare/LLMs/MiniCPM-2B-dpo-bf16/'
        llm = self.load_llm().to(self.device)
        self.useless = nn.Parameter(torch.zeros(1,requires_grad=True))
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################
    
    def load_llm(self):
        ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        from transformers import AutoModelForCausalLM,AutoTokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.path)
        self.tokenizer.add_special_tokens({'pad_token': '<pad>'})
        return AutoModelForCausalLM.from_pretrained(self.path,trust_remote_code=True,torch_dtype=torch.bfloat16).requires_grad_(False)
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################

    def logits(self, **kwargs):
        """
        Compute the logits for the input data.

        Args:
            kwargs (dict): Custom keyword arguments containing input data.
                          Modify the input processing according to your needs.

        Returns:
            logits (tensor): Logits generated by your model given input.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        def format_choices(r:list[str]):
            out = []
            for i,c in enumerate(r):
                c = c.replace('A','').replace('B','').replace('C','').replace('D','').replace('.','').strip()
                out.append('ABCD'[i]+'. '+c)
            return '\n\t\t'.join(out)
        def cnt_sentence(s:str):
            def cnt_num(s:str,c:str):
                return len(s.split(c))-1
            return sum([cnt_num(s,c) for c in ['。','！','？']])
        logits = []
        for i in range(len(kwargs['texts'])):
            text = kwargs['texts'][i]
            inputs = f"""
        我在阅读一个文段：
        -----------
        {text}
        -----------
        我首先数了一下：
        -----------
        这段话共有{cnt_sentence(text)}句话。
        -----------
        我在思考这个问题：
                {(kwargs['questions'][i])}
        我有几个可能的选项：
                {format_choices(kwargs['choices'][i])}
        从A,B,C,D中，我决定选择：""" 
            # print(inputs)
            source = self.tokenizer.encode(inputs, padding=True)
            source = torch.tensor(source).to(self.device).unsqueeze(0)
            logit = llm(source).logits[0]
            # print(logit)
            # print(self.tokenizer.decode(logit.argmax(-1).tolist()))
            logit = logit[-1]
            l_logits = [logit[95353],logit[95378],logit[95357],logit[95371]]
            # print(l_logits)
            # exit()
            logits.append(l_logits)
        return F.softmax(torch.tensor(logits),dim=1)
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################

    def get_loss(self, **kwargs):
        """
        Compute the loss for the input data and target labels.

        Args:
            kwargs (dict): Custom keyword arguments containing input data and
                           target labels. Modify the input and target processing
                           according to your needs.

        Returns:
            loss (tensor): The loss for the input data and target labels.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        targets = kwargs['targets']
        logits = self.logits(**kwargs)
        return F.cross_entropy(logits, targets)
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################
